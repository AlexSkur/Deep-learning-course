# Deep learnig course at university

### Lab 1
 Implementation of several activation functions, their derivatives and loss functions with plotting corresponding them graphics.


### Lab 2
Building a Parameter class inspired by Andrej Karpathy's micrograd.

Implementation of auto backpropagation by using topological sort.

Implementation the activations functions (2 of them) from lab 1.

Implementation of gradient descent as a function separate from the class Parameter.


### Lab 3
Creating own MLP class in which you can configure the number of hidden layers, the size of hidden layers and the activations used for each layer. Also building Dataset, DataLoader and training loop for classification task.


### Lab 4
Added BatchNorm and Dropout as a parameters which are optional for each layer of MLP class from Lab 3.
